#!/bin/bash
set -Exeuo pipefail

# This script is run multiple times in parallel: once for
# each testsuite defined in the YAML file.

THIS_DIR=$(dirname $0)

source $THIS_DIR/utils/common.sh

main() {

    # NB: bash trickery: don't use any of the function calls
    # in if-statements, it will completely disable set -e
    # inside the function... Yet another reason to port this
    # to Python.

    # NB2: if you need to change directory, do it in a
    # subshell.

    # NB3: the use of eval is strictly forbidden. Never
    # directly run a user-provided variable.

    # We take a single argument; the state dir index to use.
    # But we still expect the global state dir to be the
    # $PWD.
    state_idx=$1; shift

    state=state/suite-${state_idx}

    [ -d state ] && [ -d $state ]

    # Make sure we update GitHub if we exit due to errexit.
    # We also do a GitHub update on clean exit.
    ensure_err_github_update

    provision_env

    prepare_env

    build_and_test

    fetch_artifacts

    s3_upload

    final_github_update
}

provision_env() {
    if containerized; then
        ensure_teardown_container
        provision_container
    else
        ensure_teardown_node
        provision_node
    fi
}

provision_container() {
    local image=$(cat $state/parsed/image)

    # Let's pre-pull the image so that it doesn't count
    # as part of the test timeout.
    if ! sudo docker pull "$image"; then
        update_github error "Could not pull image '$image'."
        exit 0
    fi

    sudo docker run -d \
        --cidfile $state/cid \
        "$image" sleep infinity
}

provision_node() {

    # the allowed fields for "distro" are the same as the image name in glance
    local image=$(cat $state/parsed/distro)

    update_github pending "Provisioning test node..."

    # substitute node to use in "<name>:<ip>" format
    if [ -n "${RHCI_DEBUG_USE_NODE:-}" ]; then
        echo "${RHCI_DEBUG_USE_NODE%:*}" > $state/node_name
        echo "${RHCI_DEBUG_USE_NODE#*:}" > $state/node_addr
    else
        # XXX: We hardcode m1.small for now, but these really
        # should be specified indirectly from the .redhat-ci
        # YAML file through e.g. min-* vars.
        env \
            os_image="$image" \
            os_flavor=m1.small \
            os_name_prefix=github-ci-testnode \
            os_user_data="$THIS_DIR/utils/user-data" \
            "$THIS_DIR/utils/os_provision.py" $state
    fi

    ssh_setup_key

    ssh_wait

    if [ -f $state/parsed/ostree_revision ]; then
        if ! on_atomic_host; then
            update_github error "Cannot specify 'ostree' on non-AH."
            exit 0
        fi
        deploy_ostree
    fi
}

prepare_env() {
    local upload_dir=$state/$github_commit.$state_idx.$RANDOM
    echo $upload_dir > $state/upload_dir
    mkdir $upload_dir

    # inject extra repos before installing packages
    if [ -f $state/parsed/rhci-extras.repo ]; then
        envcmd mkdir -p /etc/yum.repos.d
        envcp $state/parsed/rhci-extras.repo /etc/yum.repos.d
    fi

    if [ -f $state/parsed/packages ]; then
        if on_atomic_host; then
            overlay_packages
        else
            install_packages
        fi
    fi

    envcmd mkdir /var/tmp/checkout
    envcp checkouts/$github_repo/. /var/tmp/checkout
}

ssh_setup_key() {
    set +x
    cat > $state/node_key <<< "$os_privkey"
    chmod 0600 $state/node_key
    set -x
}

ssh_wait() {
    local node_addr=$(cat $state/node_addr)

    timeout 120s "$THIS_DIR/utils/sshwait" $node_addr

    # We have to be extra cautious here -- OpenStack
    # networking takes some time to settle, so we wait until
    # we can contact the node for 5 continuous seconds.

    local max_sleep=30
    local failed=1

    sustain_true() {
        local sustain=5
        while [ $sustain -gt 0 ]; do
            if ! vmssh true; then
                return 1
            fi
            sustain=$((sustain - 1))
            max_sleep=$((max_sleep - 1))
            sleep 1
        done
        failed=0
    }

    while ! sustain_true && [ $max_sleep -gt 0 ]; do
        max_sleep=$((max_sleep - 1))
        sleep 1
    done

    unset -f sustain_true

    if [ $failed == 1 ]; then
        echo "ERROR: Timed out while waiting for SSH."
        return 1
    fi
}

deploy_ostree() {
    local remote=$(cat $state/parsed/ostree_remote)
    local branch=$(cat $state/parsed/ostree_branch)
    local revision=$(cat $state/parsed/ostree_revision)

    skip_reboot=0
    if [ -z "$remote" ] && [ -z "$branch" ]; then

        rc=0
        if [ -z "$revision" ]; then
            vmssh rpm-ostree upgrade --upgrade-unchanged-exit-77 || rc=$?
        else
            vmssh rpm-ostree deploy "$revision" || rc=$?
        fi

        if [ $rc == 77 ]; then
            skip_reboot=1
        elif [ $rc != 0 ]; then
            update_github error "Failed to upgrade or deploy."
            exit 0
        fi
    else
        local refspec

        if [ -n "$remote" ]; then
            vmssh ostree remote add --no-gpg-verify rhci "$remote"
            refspec=rhci:
        fi

        if [ -n "$branch" ]; then
            refspec="${refspec}$branch"
        fi

        vmssh rpm-ostree rebase "$refspec"

        if [ -n "$revision" ]; then
            # we should really be able to do this in a single step
            # https://github.com/projectatomic/rpm-ostree/issues/212
            vmreboot
            vmssh rpm-ostree deploy "$revision" || rc=$?

            if [ $rc == 77 ]; then
                skip_reboot=1
            elif [ $rc != 0 ]; then
                update_github error "Failed to upgrade or deploy."
                exit 0
            fi
        fi
    fi

    if [ $skip_reboot != 1 ]; then
        vmreboot
    fi
}

overlay_packages() {
    local upload_dir=$(cat $state/upload_dir)

    local rc=0
    logged_envcmd $upload_dir/setup.log / - - \
        rpm-ostree install "$(cat $state/parsed/packages)" || rc=$?

    if [ $rc != 0 ]; then
        s3_upload
        update_github error "Could not layer packages." "$(cat $state/url)"
        exit 0
    fi

    vmreboot
}

install_packages() {
    local upload_dir=$(cat $state/upload_dir)

    mgr=yum
    if envcmd rpm -q dnf; then
        mgr=dnf
    fi

    # This is hacky and sad. Preemptively try to refresh
    # yum/dnf cache to work around flaky distro infras.

    local retries=5
    while [ $retries -gt 0 ]; do
        if envcmd $mgr makecache; then
            break
        fi
        retries=$((retries - 1))
    done

    local rc=0
    logged_envcmd $upload_dir/setup.log / - - \
        $mgr install -y "$(cat $state/parsed/packages)" || rc=$?

    if [ $rc != 0 ]; then
        s3_upload
        update_github error "Could not install packages." "$(cat $state/url)"
        exit 0
    fi
}

run_loop() {
    local timeout=$1; shift
    local logfile=$1; shift
    local workdir=$1; shift
    local testfile=$1; shift
    local envfile=$1; shift

    local max_date=$(($(date +%s) + $timeout))
    while IFS='' read -r line || [[ -n $line ]]; do

        timeout=$(($max_date - $(date +%s)))
        if [ $timeout -le 0 ]; then
            echo "### TIMED OUT" >> $logfile
            rc=137
            break
        fi

        rc=0
        logged_envcmd $logfile $workdir $envfile $timeout "$line" || rc=$?

        if [ $rc != 0 ]; then
            break
        fi

    done < "$testfile"

    return $rc
}

build_and_test() {
    local upload_dir=$(cat $state/upload_dir)
    local timeout=$(cat $state/parsed/timeout)
    local checkout=checkouts/$github_repo
    local rc=0

    if [ -f $state/parsed/build ]; then
        local config_opts=$(cat $state/parsed/build.config_opts)
        local build_opts=$(cat $state/parsed/build.build_opts)
        local install_opts=$(cat $state/parsed/build.install_opts)

        update_github pending "Building..."

        touch $state/build.sh
        if [ ! -f $checkout/configure ]; then
            if [ -f $checkout/autogen.sh ]; then
                echo "NOCONFIGURE=1 ./autogen.sh" >> $state/build.sh
            elif [ -f $checkout/autogen ]; then
                echo "NOCONFIGURE=1 ./autogen" >> $state/build.sh
            fi
        fi

        local njobs=$(envcmd getconf _NPROCESSORS_ONLN)

        echo "./configure $config_opts" >> $state/build.sh
        echo "make all --jobs $njobs $build_opts" >> $state/build.sh
        echo "make install $install_opts" >> $state/build.sh

        local max_date=$(($(date +%s) + $timeout))

        run_loop \
            $timeout \
            $upload_dir/build.log \
            /var/tmp/checkout \
            $state/build.sh \
            $state/parsed/envs || rc=$?

        timeout=$(($max_date - $(date +%s)))
    fi

    if [ $rc = 0 ] && [ -f $state/parsed/tests ]; then

      update_github pending "Running tests..."

      run_loop \
          $timeout \
          $upload_dir/output.log \
          /var/tmp/checkout \
          $state/parsed/tests \
          $state/parsed/envs || rc=$?
    fi

    echo "$rc" > $state/rc
}

fetch_artifacts() {
    local upload_dir=$(cat $state/upload_dir)

    # let's pull back the artifacts
    if [ -f $state/parsed/artifacts ]; then

        mkdir $upload_dir/artifacts

        local fetched_at_least_one=0
        if ! containerized; then
            local node_addr=$(cat $state/node_addr)

            while IFS='' read -r artifact || [[ -n $artifact ]]; do
                path="/var/tmp/checkout/$artifact"
                if vmssh [ -e "$path" ]; then
                    vmscp -r "root@$node_addr:$path" $upload_dir/artifacts
                    fetched_at_least_one=1
                fi
            done < $state/parsed/artifacts
        else
            local cid=$(cat $state/cid)
            while IFS='' read -r artifact || [[ -n $artifact ]]; do
                path="/var/tmp/checkout/$artifact"
                if sudo docker exec $cid [ -e "$path" ]; then
                    sudo docker cp "$cid:$path" $upload_dir/artifacts
                    fetched_at_least_one=1
                fi
            done < $state/parsed/artifacts
        fi

        if [ $fetched_at_least_one == 0 ]; then
            rm -rf $upload_dir/artifacts
        fi
    fi
}

s3_upload() {
    local indexer=$(realpath $THIS_DIR/utils/indexer.py)
    local upload_dir=$(cat $state/upload_dir)
    local s3_object=index.html

    # if we just have output.log or build.log, then just link directly to that
    if [ $(find $upload_dir -mindepth 1 | wc -l) = 1 ]; then
        if [ -f $upload_dir/output.log ]; then
            s3_object=output.log
        elif [ -f $upload_dir/build.log ]; then
            s3_object=build.log
        fi
    fi

    if [ $s3_object = index.html ]; then
        # don't change directory in current session
        ( cd $upload_dir && $indexer )
    fi

    # only actually upload if we're given $s3_prefix
    if [ -n "${s3_prefix:-}" ]; then

        local full_prefix=$s3_prefix/$github_repo/$(basename $upload_dir)

        # upload logs separately so that we can set the MIME type properly
        aws s3 sync --exclude '*.log' \
            $upload_dir s3://$full_prefix
        aws s3 sync --exclude '*' --include '*.log' --content-type text/plain \
            $upload_dir s3://$full_prefix

        # full address we'll use for the final commit status update
        printf "https://s3.amazonaws.com/%s/%s" \
            $full_prefix $s3_object > $state/url
    fi
}

final_github_update() {
    local rc
    local ghstate
    local desc

    rc=$(cat $state/rc)
    if [ $rc == 124 ] || [ $rc == 137 ]; then
        ghstate=failure
        desc="Test timed out and was aborted."
    elif [ $rc != 0 ]; then
        ghstate=failure
        desc="Test failed with rc $rc."
    else
        ghstate=success
        desc="All tests passed"
        if [ -n "${github_pull_id:-}" ] && [ ! -f state/is_merge_sha ]; then
            desc="$desc, but merge commit could not be tested"
        fi
        desc="${desc}."
    fi

    local url=
    if [ -f $state/url ]; then
        url=$(cat $state/url)
    fi

    update_github $ghstate "$desc" "$url"
}

# $1 -- log file
# $2 -- workdir
# $3 -- envfile or -
# $4 -- timeout or -
logged_envcmd() {
    local logfile=$1; shift
    local workdir=$1; shift
    local envfile=$1; shift
    local timeout=$1; shift

    # seed with standard info
    if [ ! -f $logfile ]; then

        echo "### $(date --utc)" > $logfile

        if [ -n "${github_branch:-}" ]; then
            echo "### Branch $github_branch" >> $logfile
        else
            echo -n "### PR #$github_pull_id" >> $logfile
            # NB: is_merge_sha is in the top-level global state dir
            if [ ! -f state/is_merge_sha ]; then
                echo " (WARNING: not merge sha, check for conflicts)" \
                    >> $logfile
            else
                echo >> $logfile
            fi
        fi

        if [ -n "${BUILD_ID:-}" ]; then
            echo "### BUILD_ID $BUILD_ID" >> $logfile
        fi
    fi

    echo '>>>' "$@" >> $logfile

    # we just create a script and run that to make
    # invocation and redirection easier
    echo "set -euo pipefail" > $state/worker.sh
    if [ $envfile != - ] && [ -f $envfile ]; then
        cat $envfile >> $state/worker.sh
    fi
    echo "exec 2>&1" >> $state/worker.sh
    echo "cd $workdir" >> $state/worker.sh
    echo "$@" >> $state/worker.sh

    envcp $state/worker.sh /var/tmp

    local start=$(date +%s)

    rc=0
    timed_envcmd $timeout sh /var/tmp/worker.sh | tee -a $logfile || rc=$?

    local duration=$(($(date +%s) - $start))

    if [ $rc == 0 ]; then
        echo "### COMPLETED IN ${duration}s" >> $logfile
    elif [ $rc == 137 ]; then
        echo "### TIMED OUT AFTER ${duration}s" >> $logfile
    else
        echo "### EXITED WITH CODE $rc AFTER ${duration}s" >> $logfile
    fi

    return $rc
}

# $1 -- timeout or -
timed_envcmd() {
    timeout=$1; shift

    if [[ $timeout == - ]]; then
        timeout=infinity
    fi

    # There's a tricky bit to note here, docker exec and ssh
    # don't handle arguments exactly the same way. SSH will
    # effectively do a 'sh -c' on the passed command, which
    # means that quoting might be an issue.

    if containerized; then
        local cid=$(cat $state/cid)
        sudo timeout --signal=KILL $timeout \
            docker exec $cid "$@"
    else
        local node_addr=$(cat $state/node_addr)
        timeout --signal=KILL $timeout \
            ssh -q -n -i $state/node_key \
                -o StrictHostKeyChecking=no \
                -o PasswordAuthentication=no \
                -o UserKnownHostsFile=/dev/null \
                root@$(cat $state/node_addr) "$@"
    fi
}

envcmd() {
    timed_envcmd infinity "$@"
}

envcp() {
    target=$1; shift
    remote=$1; shift

    # N.B.: docker cp and rsync have almost the same
    # semantics, which is nice. One exception is that docker
    # wants 'dir/.' to signify copying dir contents, whereas
    # rsync is happy with just 'dir/', so always use '/.'.
    # Also, rsync creates nonexistent dirs, whereas docker
    # does not, so explicitly mkdir beforehand.

    if containerized; then
        local cid=$(cat $state/cid)
        sudo docker cp $target $cid:$remote
    else
        local node_addr=$(cat $state/node_addr)
        rsync --quiet -az --no-owner --no-group \
            -e "ssh -q -i $state/node_key \
                       -o StrictHostKeyChecking=no \
                       -o PasswordAuthentication=no \
                       -o UserKnownHostsFile=/dev/null" \
            $target root@$node_addr:$remote
    fi
}

vmssh() {
    # NB: we use -n because stdin may be in use (e.g. in a
    # bash while read loop)
    ssh -q -n -i $state/node_key \
        -o StrictHostKeyChecking=no \
        -o PasswordAuthentication=no \
        -o UserKnownHostsFile=/dev/null \
        root@$(cat $state/node_addr) "$@"
}

vmscp() {
    scp -q -i $state/node_key \
        -o StrictHostKeyChecking=no \
        -o PasswordAuthentication=no \
        -o UserKnownHostsFile=/dev/null "$@"
}

vmreboot() {
    vmssh systemctl reboot || :
    sleep 3 # give time for port to go down
    ssh_wait
}

update_github() {
    local context=$(cat $state/parsed/context)
    common_update_github "$context" "$@"
}

ensure_err_github_update() {
    trap "update_github error 'An internal error occurred.'" ERR
}

teardown_node() {

    if [ -f $state/node_name ]; then

        local node_name=$(cat $state/node_name)
        local node_addr=$(cat $state/node_addr)

        if [ -f $state/node_addr ] && \
           [ -n "${os_floating_ip_pool:-}" ]; then
            nova floating-ip-disassociate $node_name $node_addr
            nova floating-ip-delete $node_addr
        fi

        nova delete $(cat $state/node_name)
    fi
}

ensure_teardown_node() {
    if [ -z "${RHCI_DEBUG_NO_TEARDOWN:-}" ]; then
        trap teardown_node EXIT
    fi
}

teardown_container() {
    if [ -f $state/cid ]; then
        sudo docker rm -f $(cat $state/cid)
    fi
}

ensure_teardown_container() {
    if [ -z "${RHCI_DEBUG_NO_TEARDOWN:-}" ]; then
        trap teardown_container EXIT
    fi
}

containerized() {
    [ -f $state/parsed/image ]
}

on_atomic_host() {
    ! containerized && vmssh test -f /run/ostree-booted
}

main "$@"
